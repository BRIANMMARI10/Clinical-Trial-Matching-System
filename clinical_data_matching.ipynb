{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "2SOW0VeL7-6p"
      },
      "outputs": [],
      "source": [
        "# Download all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords once\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")  # Needed for Word2Vec tokenization\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DBAqEo_h-UpO",
        "outputId": "1bec5182-8c9a-4e96-8037-b83b633ce3ad"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text (applied at the earliest point)\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by converting to lowercase, removing punctuation, and removing stopwords.\"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "cCLc2E7c-So7"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data = pd.read_csv('/content/patients.csv')\n",
        "\n",
        "# Extract relevant attributes\n",
        "patient_data = patient_data[['Id', 'BIRTHDATE', 'GENDER']]\n",
        "\n",
        "# Convert birthdate to age\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_age(birthdate):\n",
        "    birthdate = datetime.strptime(birthdate, \"%Y-%m-%d\")\n",
        "    today = datetime.today()\n",
        "    return today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))"
      ],
      "metadata": {
        "id": "xThrhfH08Cd3"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data['AGE'] = patient_data['BIRTHDATE'].apply(calculate_age)\n",
        "\n",
        "# Ensure Id is clean and string type\n",
        "patient_data['Id'] = patient_data['Id'].astype(str).str.strip()\n",
        "\n",
        "diagnosis_data = pd.read_csv('/content/conditions.csv')\n",
        "# Ensure Patient is clean and string type\n",
        "diagnosis_data['PATIENT'] = diagnosis_data['PATIENT'].astype(str).str.strip()\n",
        "\n",
        "# Aggregate conditions for each patient\n",
        "patient_conditions = diagnosis_data.groupby('PATIENT')['DESCRIPTION'].apply(list).reset_index()\n",
        "\n",
        "# Merge patient conditions\n",
        "patient_data = patient_data.merge(patient_conditions, left_on='Id', right_on='PATIENT', how='left')\n",
        "\n",
        "# Convert NaN conditions to empty lists\n",
        "patient_data['DESCRIPTION'] = patient_data['DESCRIPTION'].apply(lambda x: x if isinstance(x, list) else [])\n"
      ],
      "metadata": {
        "id": "BMOla6iL_NUu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the merge\n",
        "print(patient_data.head())\n",
        "print(patient_data.isnull().sum())\n",
        "\n",
        "output_file = \"merged_patient_data.csv\"\n",
        "patient_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"CSV file saved as: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tabjjPNy_miy",
        "outputId": "2cf4a610-7215-49a4-a3ba-223760bc21d4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Id   BIRTHDATE GENDER  AGE  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46  1994-02-06      M   31   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f  1968-08-06      M   56   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668  2008-12-21      M   16   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876  1994-01-27      F   31   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405  2019-07-27      M    5   \n",
            "\n",
            "                                PATIENT  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405   \n",
            "\n",
            "                                         DESCRIPTION  \n",
            "0  [Housing unsatisfactory (finding), Received hi...  \n",
            "1  [Serving in military service (finding), Receiv...  \n",
            "2  [Medication review due (situation), Traumatic ...  \n",
            "3  [Chronic intractable migraine without aura (di...  \n",
            "4  [Medication review due (situation), Medication...  \n",
            "Id             0\n",
            "BIRTHDATE      0\n",
            "GENDER         0\n",
            "AGE            0\n",
            "PATIENT        0\n",
            "DESCRIPTION    0\n",
            "dtype: int64\n",
            "CSV file saved as: merged_patient_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Clean Patient Data EARLY\n",
        "patient_file = \"merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# Convert DESCRIPTION column to list if stored as a string\n",
        "import ast\n",
        "patients_df['DESCRIPTION'] = patients_df['DESCRIPTION'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Clean patient descriptions **before merging**\n",
        "patients_df['DESCRIPTION'] = patients_df['DESCRIPTION'].apply(lambda desc: [clean_text(cond) for cond in desc])\n",
        "\n",
        "# Merge conditions into a single cleaned text string per patient\n",
        "patients_df['combined_conditions'] = patients_df['DESCRIPTION'].apply(lambda x: \" \".join(x))"
      ],
      "metadata": {
        "id": "ZfqFVDJ6_rPU"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and Clean XML Trial Data EARLY\n",
        "xml_files = glob.glob(\"/content/NCT*.xml\")\n",
        "\n",
        "def extract_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract eligibility criteria **and clean it immediately**\n",
        "    eligibility_criteria = root.find(\".//eligibility/criteria/textblock\")\n",
        "    eligibility_text = clean_text(eligibility_criteria.text) if eligibility_criteria is not None else \"\"\n",
        "\n",
        "    # Extract and clean age range & gender requirements\n",
        "    min_age = root.find(\".//eligibility/minimum_age\")\n",
        "    max_age = root.find(\".//eligibility/maximum_age\")\n",
        "    gender = root.find(\".//eligibility/gender\")\n",
        "\n",
        "    min_age = clean_text(min_age.text) if min_age is not None else \"0 years\"\n",
        "    max_age = clean_text(max_age.text) if max_age is not None else \"100 years\"\n",
        "    gender = clean_text(gender.text) if gender is not None else \"all\"\n",
        "\n",
        "    # Convert age range into numerical values\n",
        "    def extract_age(age_text):\n",
        "        return int(age_text.split()[0]) if \"years\" in age_text else 0\n",
        "\n",
        "    min_age = extract_age(min_age)\n",
        "    max_age = extract_age(max_age)\n",
        "\n",
        "    # Extract inclusion and exclusion criteria (and clean them)\n",
        "    inclusion_criteria = []\n",
        "    exclusion_criteria = []\n",
        "    parsing_exclusion = False\n",
        "\n",
        "    for line in eligibility_text.split(\"\\n\"):\n",
        "        line = clean_text(line.strip())  # Ensure each line is cleaned\n",
        "        if \"exclusion\" in line:\n",
        "            parsing_exclusion = True\n",
        "        elif \"inclusion\" in line:\n",
        "            parsing_exclusion = False\n",
        "        elif line:\n",
        "            if parsing_exclusion:\n",
        "                exclusion_criteria.append(line)\n",
        "            else:\n",
        "                inclusion_criteria.append(line)\n",
        "\n",
        "    return min_age, max_age, gender, inclusion_criteria, exclusion_criteria"
      ],
      "metadata": {
        "id": "AqYx5eWi_4nC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Process String-Based Matching on Cleaned Data\n",
        "def is_eligible(patient, min_age, max_age, gender, inclusion_criteria, exclusion_criteria):\n",
        "    \"\"\"Check if a patient is eligible for a trial based on cleaned conditions and criteria.\"\"\"\n",
        "    # Check age\n",
        "    if not (min_age <= patient[\"AGE\"] <= max_age):\n",
        "        return False\n",
        "\n",
        "    # Check gender\n",
        "    if gender != \"All\" and patient[\"GENDER\"] != gender:\n",
        "        return False\n",
        "\n",
        "    # Clean inclusion/exclusion criteria\n",
        "    inclusion_criteria = [clean_text(inc) for inc in inclusion_criteria]\n",
        "    exclusion_criteria = [clean_text(exc) for exc in exclusion_criteria]\n",
        "\n",
        "    # Convert patient conditions to cleaned string\n",
        "    patient_conditions = \" \".join(patient[\"DESCRIPTION\"]).lower()\n",
        "\n",
        "    # Inclusion: At least one criterion should match\n",
        "    if not any(any(word in patient_conditions for word in inc.split()) for inc in inclusion_criteria):\n",
        "        return False  # No match\n",
        "\n",
        "    # Exclusion: No disqualifying criteria should be present\n",
        "    if any(exc in patient_conditions for exc in exclusion_criteria):\n",
        "        return False  # Disqualified\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "NO4iEWs6_8dI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 1: Process each XML file\n",
        "all_eligible_patients = []\n",
        "if all_eligible_patients:\n",
        "    final_eligible_patients_df = pd.concat(all_eligible_patients, ignore_index=True)\n",
        "    final_eligible_patients_df = final_eligible_patients_df.groupby([\"Id\", \"AGE\", \"GENDER\"])[\"Trial_ID\"].apply(list).reset_index()\n",
        "\n",
        "    # Save to file\n",
        "    output_file = \"eligible_patients.csv\"\n",
        "    final_eligible_patients_df.to_csv(output_file, index=False)\n",
        "    print(f\"Eligible patients saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No eligible patients found. Skipping CSV export.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aKD4CdiDB8_N",
        "outputId": "788ecd1c-96ca-470d-ade4-3b633724bf17"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No eligible patients found. Skipping CSV export.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 2: Word2Vec Training on Cleaned Data\n",
        "sentences = [word_tokenize(\" \".join(desc)) for desc in patients_df['DESCRIPTION']]\n",
        "\n",
        "# Add cleaned inclusion criteria for trials\n",
        "for xml_file in xml_files:\n",
        "    _, _, _, inclusion_criteria, _ = extract_criteria(xml_file)  # Extract only inclusion criteria\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(word_tokenize(\" \".join(inclusion_criteria)))  # Convert list to string before tokenizing\n",
        "\n",
        "# Train Word2Vec on cleaned text\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model.save(\"word2vec_patient_trials.model\")"
      ],
      "metadata": {
        "id": "2gz-rCpgACWx"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 2: Compute Patient & Trial Embeddings\n",
        "def get_w2v_embedding(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # Use zero vector if no words found\n",
        "\n",
        "# Compute embeddings for all patients and trials\n",
        "patients_df[\"w2v_embedding\"] = patients_df[\"combined_conditions\"].apply(get_w2v_embedding)\n",
        "trials_df[\"w2v_embedding\"] = trials_df[\"text_cleaned\"].apply(get_w2v_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"w2v_embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"w2v_embedding\"].values)\n",
        "\n",
        "# Step 5: Compute Cosine Similarity\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4  # Match the BERT threshold\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n"
      ],
      "metadata": {
        "id": "yDxw8Iv3ClYJ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 2: Construct the Output in eligible Format\n",
        "matched_patients_w2v = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ],
      "metadata": {
        "id": "8i6QsLRaJWxc"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 2: Save Results in BERT Format\n",
        "output_df_w2v = pd.DataFrame(matched_patients_w2v)\n",
        "output_df_w2v.to_excel(\"word2vec_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"word2vec_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients_w2v, json_file, indent=4)\n",
        "\n",
        "print(f\"Word2Vec-based matching completed. {len(matched_patients_w2v)} patient-trial pairs found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6EXqoAgLJSQI",
        "outputId": "07927f1c-0300-41c5-c868-9e069c79cae0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec-based matching completed. 19398 patient-trial pairs found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all XML files\n",
        "def extract_inclusion_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "    text = root.find(\".//eligibility/criteria/textblock\")\n",
        "    return word_tokenize(text.text.lower()) if text is not None else []\n",
        "\n",
        "# Add inclusion criteria to Word2Vec training data\n",
        "for xml_file in xml_files:\n",
        "    inclusion_criteria = extract_inclusion_criteria(xml_file)\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(inclusion_criteria)  # Include trial criteria in training data"
      ],
      "metadata": {
        "id": "yVp6zkl9GHSk"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 3: Compute BERT Embeddings on Fully Cleaned Data\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Returns the BERT embedding of the given text as a NumPy array.\"\"\"\n",
        "    if not text:\n",
        "        return np.zeros(bert_model.get_sentence_embedding_dimension())\n",
        "    return bert_model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "# Compute embeddings on cleaned text\n",
        "patients_df[\"embedding\"] = patients_df[\"combined_conditions\"].apply(get_bert_embedding)\n",
        "trials_df[\"embedding\"] = trials_df[\"text_cleaned\"].apply(get_bert_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"embedding\"].values)"
      ],
      "metadata": {
        "id": "LBvfqXrLAMHO"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 3: Compute Similarity (Using Fully Cleaned Data)\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4  # Adjusted from 0.6 to improve matching\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n",
        "# Construct the matched patients list\n",
        "matched_patients = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ],
      "metadata": {
        "id": "oo_7HTt6AOUH"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy 3: Save Results\n",
        "output_df = pd.DataFrame(matched_patients)\n",
        "output_df.to_excel(\"bert_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"bert_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients, json_file, indent=4)\n",
        "\n",
        "print(f\"BERT-based matching completed. {len(matched_patients)} patient-trial pairs found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h7y48qNWAQH_",
        "outputId": "aa759bc7-f8cb-4fe2-8b6f-36f8e51fbd61"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT-based matching completed. 1763 patient-trial pairs found.\n"
          ]
        }
      ]
    }
  ]
}