{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.49.0)\n",
            "Requirement already satisfied: tqdm in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: scipy in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.29.2)\n",
            "Requirement already satisfied: Pillow in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "2SOW0VeL7-6p"
      },
      "outputs": [],
      "source": [
        "# Download all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import ast\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DBAqEo_h-UpO",
        "outputId": "1bec5182-8c9a-4e96-8037-b83b633ce3ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/brianmmari/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/brianmmari/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords once\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")  # Needed for Word2Vec tokenization\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "cCLc2E7c-So7"
      },
      "outputs": [],
      "source": [
        "# Function to clean text (applied at the earliest point)\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by converting to lowercase, removing punctuation, and removing stopwords.\"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "xThrhfH08Cd3"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = os.getcwd() \n",
        "\n",
        "patient_data = pd.read_csv(os.path.join(BASE_DIR, 'raw', 'patients.csv'))\n",
        "\n",
        "diagnosis_data = pd.read_csv(os.path.join(BASE_DIR, 'raw', 'diagnosis.csv'))\n",
        "\n",
        "# Extract relevant attributes\n",
        "patient_data = patient_data[['Id', 'BIRTHDATE', 'GENDER']]\n",
        "\n",
        "# Convert birthdate to age\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_age(birthdate):\n",
        "    birthdate = datetime.strptime(birthdate, \"%Y-%m-%d\")\n",
        "    today = datetime.today()\n",
        "    return today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "BMOla6iL_NUu"
      },
      "outputs": [],
      "source": [
        "patient_data['AGE'] = patient_data['BIRTHDATE'].apply(calculate_age)\n",
        "\n",
        "# Ensure patient IDs are properly formatted\n",
        "patient_data[\"Id\"] = patient_data[\"Id\"].astype(str).str.strip()\n",
        "diagnosis_data[\"PATIENT\"] = diagnosis_data[\"PATIENT\"].astype(str).str.strip()\n",
        "\n",
        "# Group diagnoses for each patient\n",
        "patient_conditions = diagnosis_data.groupby(\"PATIENT\")[\"DESCRIPTION\"].apply(list).reset_index()\n",
        "\n",
        "# Merge with patient data\n",
        "patient_data = patient_data.merge(patient_conditions, left_on=\"Id\", right_on=\"PATIENT\", how=\"left\")\n",
        "\n",
        "# Convert NaN to empty lists\n",
        "patient_data[\"DESCRIPTION\"] = patient_data[\"DESCRIPTION\"].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Debug: Print sample to check if descriptions exist now\n",
        "print(\"Updated patient descriptions:\", patient_data[\"DESCRIPTION\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tabjjPNy_miy",
        "outputId": "2cf4a610-7215-49a4-a3ba-223760bc21d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     Id   BIRTHDATE GENDER  AGE  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46  1994-02-06      M   31   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f  1968-08-06      M   56   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668  2008-12-21      M   16   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876  1994-01-27      F   31   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405  2019-07-27      M    5   \n",
            "\n",
            "                                PATIENT  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405   \n",
            "\n",
            "                                         DESCRIPTION  \n",
            "0  [Housing unsatisfactory (finding), Received hi...  \n",
            "1  [Serving in military service (finding), Receiv...  \n",
            "2  [Medication review due (situation), Traumatic ...  \n",
            "3  [Chronic intractable migraine without aura (di...  \n",
            "4  [Medication review due (situation), Medication...  \n",
            "Id             0\n",
            "BIRTHDATE      0\n",
            "GENDER         0\n",
            "AGE            0\n",
            "PATIENT        0\n",
            "DESCRIPTION    0\n",
            "dtype: int64\n",
            "CSV file saved as: /Users/brianmmari/Clinical-Trial-Matching-System/outputs/merged_patient_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Verify the merge\n",
        "print(patient_data.head())\n",
        "print(patient_data.isnull().sum())\n",
        "\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")  # Store outputs in a dedicated folder\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, \"merged_patient_data.csv\")\n",
        "patient_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"CSV file saved as: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "ZfqFVDJ6_rPU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/brianmmari/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and Clean Patient Data EARLY\n",
        "patient_file = \"/Users/brianmmari/Clinical-Trial-Matching-System/outputs/merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# 2. Clean the DESCRIPTION column (ensure it's a list)\n",
        "patients_df[\"DESCRIPTION\"] = patients_df[\"DESCRIPTION\"].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# 3. Download the tokenizer model\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# 4. Tokenize descriptions safely\n",
        "def safe_tokenize(desc):\n",
        "    if isinstance(desc, list):\n",
        "        return word_tokenize(\" \".join(desc))  # Tokenize only if it's a list\n",
        "    return []\n",
        "\n",
        "patients_df[\"tokenized_description\"] = patients_df[\"DESCRIPTION\"].apply(safe_tokenize)\n",
        "\n",
        "# Merge conditions into a single cleaned text string per patient\n",
        "patients_df['combined_conditions'] = patients_df['DESCRIPTION'].apply(lambda x: \" \".join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "AqYx5eWi_4nC"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load and Clean XML Trial Data EARLY\n",
        "xml_files = glob.glob(os.path.join(BASE_DIR, \"raw\", \"NCT*.xml\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'text_cleaned'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[119], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(keywords)  \u001b[39m# Return space-separated keywords\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# Step 6: Apply Medical Keyword Extraction\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(extract_medical_keywords)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'"
          ]
        }
      ],
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "# Step 1: Find All XML Files in the Directory\n",
        "xml_files = glob.glob(\"/Users/brianmmari/Clinical-Trial-Matching-System/data/*.xml\")  # Adjust path if needed\n",
        "\n",
        "# Step 2: Extract Data from XML Files\n",
        "def extract_trial_data(trial_file):\n",
        "    \"\"\"Extract relevant details from each clinical trial XML file.\"\"\"\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract trial ID\n",
        "    trial_id = root.find(\".//id_info/nct_id\")\n",
        "    trial_id = trial_id.text if trial_id is not None else \"Unknown\"\n",
        "\n",
        "    # Extract eligibility criteria\n",
        "    eligibility_criteria = root.find(\".//eligibility/criteria/textblock\")\n",
        "    eligibility_text = eligibility_criteria.text if eligibility_criteria is not None else \"\"\n",
        "\n",
        "    return {\"Trial_ID\": trial_id, \"text_cleaned\": eligibility_text}\n",
        "\n",
        "# Step 3: Process All XML Files\n",
        "trial_data = [extract_trial_data(xml) for xml in xml_files]\n",
        "\n",
        "# Step 4: Convert to DataFrame\n",
        "trials_df = pd.DataFrame(trial_data)\n",
        "\n",
        "# Step 5: Extract Medical Keywords from Trial Descriptions\n",
        "def extract_medical_keywords(text):\n",
        "    \"\"\"Extract potential medical keywords from trial descriptions.\"\"\"\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return \"\"\n",
        "\n",
        "    # Remove irrelevant phrases\n",
        "    text = text.lower().replace(\"please contact site for information.\", \"\")\n",
        "\n",
        "    # Extract words that indicate conditions (modify this as needed)\n",
        "    keywords = re.findall(r'\\b[a-zA-Z-]+\\b', text)  # Extract words\n",
        "    return \" \".join(keywords)  # Return space-separated keywords\n",
        "\n",
        "# Step 6: Apply Medical Keyword Extraction\n",
        "trials_df[\"text_cleaned\"] = trials_df[\"text_cleaned\"].apply(extract_medical_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Series([], dtype: float64)\n"
          ]
        }
      ],
      "source": [
        "print(trials_df.head())  # Check if data was extracted properly\n",
        "print(trials_df.isnull().sum())  # Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract eligibility criteria **and clean it immediately**\n",
        "    eligibility_criteria = root.find(\".//eligibility/criteria/textblock\")\n",
        "    eligibility_text = clean_text(eligibility_criteria.text) if eligibility_criteria is not None else \"\"\n",
        "\n",
        "    # Extract and clean age range & gender requirements\n",
        "    min_age = root.find(\".//eligibility/minimum_age\")\n",
        "    max_age = root.find(\".//eligibility/maximum_age\")\n",
        "    gender = root.find(\".//eligibility/gender\")\n",
        "\n",
        "    min_age = clean_text(min_age.text) if min_age is not None else \"0 years\"\n",
        "    max_age = clean_text(max_age.text) if max_age is not None else \"100 years\"\n",
        "    gender = clean_text(gender.text) if gender is not None else \"all\"\n",
        "\n",
        "    # Convert age range into numerical values\n",
        "    def extract_age(age_text):\n",
        "        return int(age_text.split()[0]) if \"years\" in age_text else 0\n",
        "\n",
        "    min_age = extract_age(min_age)\n",
        "    max_age = extract_age(max_age)\n",
        "\n",
        "    # Extract inclusion and exclusion criteria (and clean them)\n",
        "    inclusion_criteria = []\n",
        "    exclusion_criteria = []\n",
        "    parsing_exclusion = False\n",
        "\n",
        "    for line in eligibility_text.split(\"\\n\"):\n",
        "        line = clean_text(line.strip())  # Ensure each line is cleaned\n",
        "        if \"exclusion\" in line:\n",
        "            parsing_exclusion = True\n",
        "        elif \"inclusion\" in line:\n",
        "            parsing_exclusion = False\n",
        "        elif line:\n",
        "            if parsing_exclusion:\n",
        "                exclusion_criteria.append(line)\n",
        "            else:\n",
        "                inclusion_criteria.append(line)\n",
        "\n",
        "    return min_age, max_age, gender, inclusion_criteria, exclusion_criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "NO4iEWs6_8dI"
      },
      "outputs": [],
      "source": [
        "# Strategy 1: Process String-Based Matching on Cleaned Data\n",
        "def is_eligible(patient, min_age, max_age, gender, inclusion_criteria, exclusion_criteria):\n",
        "    \"\"\"Check if a patient is eligible for a trial based on cleaned conditions and criteria.\"\"\"\n",
        "    # Check age\n",
        "    if not (min_age <= patient[\"AGE\"] <= max_age):\n",
        "        return False\n",
        "\n",
        "    # Check gender\n",
        "    if gender != \"All\" and patient[\"GENDER\"] != gender:\n",
        "        return False\n",
        "\n",
        "    # Clean inclusion/exclusion criteria\n",
        "    inclusion_criteria = [clean_text(inc) for inc in inclusion_criteria]\n",
        "    exclusion_criteria = [clean_text(exc) for exc in exclusion_criteria]\n",
        "\n",
        "    # Convert patient conditions to cleaned string\n",
        "    patient_conditions = \" \".join(patient[\"DESCRIPTION\"]).lower()\n",
        "\n",
        "    # Inclusion: At least one criterion should match\n",
        "    if not any(any(word in patient_conditions for word in inc.split()) for inc in inclusion_criteria):\n",
        "        return False  # No match\n",
        "\n",
        "    # Exclusion: No disqualifying criteria should be present\n",
        "    if any(exc in patient_conditions for exc in exclusion_criteria):\n",
        "        return False  # Disqualified\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aKD4CdiDB8_N",
        "outputId": "788ecd1c-96ca-470d-ade4-3b633724bf17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No eligible patients found. Skipping CSV export.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 1: Process each XML file\n",
        "all_eligible_patients = []\n",
        "if all_eligible_patients:\n",
        "    final_eligible_patients_df = pd.concat(all_eligible_patients, ignore_index=True)\n",
        "    final_eligible_patients_df = final_eligible_patients_df.groupby([\"Id\", \"AGE\", \"GENDER\"])[\"Trial_ID\"].apply(list).reset_index()\n",
        "\n",
        "    # Save to file\n",
        "    output_file = \"eligible_patients.csv\"\n",
        "    final_eligible_patients_df.to_csv(output_file, index=False)\n",
        "    print(f\"Eligible patients saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No eligible patients found. Skipping CSV export.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "2gz-rCpgACWx"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "you must first build vocabulary before training the model",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[124], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         sentences\u001b[39m.\u001b[39mappend(word_tokenize(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(inclusion_criteria)))  \u001b[39m# Convert list to string before tokenizing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Train Word2Vec on cleaned text\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m w2v_model \u001b[39m=\u001b[39m Word2Vec(sentences, vector_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m     12\u001b[0m w2v_model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mword2vec_patient_trials.model\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1045\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha \u001b[39m=\u001b[39m end_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m epochs\n\u001b[0;32m-> 1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_training_sanity(epochs\u001b[39m=\u001b[39mepochs, total_examples\u001b[39m=\u001b[39mtotal_examples, total_words\u001b[39m=\u001b[39mtotal_words)\n\u001b[1;32m   1046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39mepochs)\n\u001b[1;32m   1048\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m   1049\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1050\u001b[0m     msg\u001b[39m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     ),\n\u001b[1;32m   1055\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/word2vec.py:1543\u001b[0m, in \u001b[0;36mWord2Vec._check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mEffective \u001b[39m\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m\u001b[39m higher than previous training cycles\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mkey_to_index:  \u001b[39m# should be set by `build_vocab`\u001b[39;00m\n\u001b[0;32m-> 1543\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must first build vocabulary before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1544\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvectors):\n\u001b[1;32m   1545\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39myou must initialize vectors before training the model\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ],
      "source": [
        "# Strategy 2: Word2Vec Training on Cleaned Data\n",
        "sentences = [word_tokenize(\" \".join(desc)) for desc in patients_df['DESCRIPTION']]\n",
        "\n",
        "# Add cleaned inclusion criteria for trials\n",
        "for xml_file in xml_files:\n",
        "    _, _, _, inclusion_criteria, _ = extract_criteria(xml_file)  # Extract only inclusion criteria\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(word_tokenize(\" \".join(inclusion_criteria)))  # Convert list to string before tokenizing\n",
        "\n",
        "# Train Word2Vec on cleaned text\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model.save(\"word2vec_patient_trials.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample patient descriptions: 0    []\n",
            "1    []\n",
            "2    []\n",
            "3    []\n",
            "4    []\n",
            "Name: DESCRIPTION, dtype: object\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'text_cleaned'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[109], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSample patient descriptions:\u001b[39m\u001b[39m\"\u001b[39m, patients_df[\u001b[39m\"\u001b[39m\u001b[39mDESCRIPTION\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhead())\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSample trial eligibility criteria:\u001b[39m\u001b[39m\"\u001b[39m, trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhead())\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'"
          ]
        }
      ],
      "source": [
        "print(\"Sample patient descriptions:\", patients_df[\"DESCRIPTION\"].head())\n",
        "print(\"Sample trial eligibility criteria:\", trials_df[\"text_cleaned\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "yDxw8Iv3ClYJ"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'text_cleaned'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[107], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Compute embeddings for all patients and trials\u001b[39;00m\n\u001b[1;32m      8\u001b[0m patients_df[\u001b[39m\"\u001b[39m\u001b[39mw2v_embedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m patients_df[\u001b[39m\"\u001b[39m\u001b[39mcombined_conditions\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(get_w2v_embedding)\n\u001b[0;32m----> 9\u001b[0m trials_df[\u001b[39m\"\u001b[39m\u001b[39mw2v_embedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(get_w2v_embedding)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Convert embeddings to NumPy arrays\u001b[39;00m\n\u001b[1;32m     12\u001b[0m patient_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(patients_df[\u001b[39m\"\u001b[39m\u001b[39mw2v_embedding\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_cleaned'"
          ]
        }
      ],
      "source": [
        "# Strategy 2: Compute Patient & Trial Embeddings\n",
        "def get_w2v_embedding(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # Use zero vector if no words found\n",
        "\n",
        "# Compute embeddings for all patients and trials\n",
        "patients_df[\"w2v_embedding\"] = patients_df[\"combined_conditions\"].apply(get_w2v_embedding)\n",
        "trials_df[\"w2v_embedding\"] = trials_df[\"text_cleaned\"].apply(get_w2v_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"w2v_embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"w2v_embedding\"].values)\n",
        "\n",
        "# Step 5: Compute Cosine Similarity\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.2  # Match the BERT threshold\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "8i6QsLRaJWxc"
      },
      "outputs": [],
      "source": [
        "# Strategy 2: Construct the Output in eligible Format\n",
        "matched_patients_w2v = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6EXqoAgLJSQI",
        "outputId": "07927f1c-0300-41c5-c868-9e069c79cae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec-based matching completed. 0 patient-trial pairs found.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 2: Save Results in BERT Format\n",
        "output_df_w2v = pd.DataFrame(matched_patients_w2v)\n",
        "output_df_w2v.to_excel(\"word2vec_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"word2vec_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients_w2v, json_file, indent=4)\n",
        "\n",
        "print(f\"Word2Vec-based matching completed. {len(matched_patients_w2v)} patient-trial pairs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "yVp6zkl9GHSk"
      },
      "outputs": [],
      "source": [
        "# Load all XML files\n",
        "def extract_inclusion_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "    text = root.find(\".//eligibility/criteria/textblock\")\n",
        "    return word_tokenize(text.text.lower()) if text is not None else []\n",
        "\n",
        "# Add inclusion criteria to Word2Vec training data\n",
        "for xml_file in xml_files:\n",
        "    inclusion_criteria = extract_inclusion_criteria(xml_file)\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(inclusion_criteria)  # Include trial criteria in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LBvfqXrLAMHO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f7b04a647c945f99cfa0580d9656abc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7741759e3a404a6581f7c187f46cd17d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fcedb7c157644b58d63da01ce38f1e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2706c68dbc7a46bc9042f1b3e8b86133",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d4778b1c7034d0ca475ffca345fc165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b6bc2912c2c4b5993e4f6cb65ff08ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2491053b774e4aa2af4fb988673c407d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8569af563104eda86622a3b7ab2f038",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9220a450395d4c5f932aa9f36b4b5152",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89494d4a6e85401ba4e429a4a5b944b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15151b143c384c3a81328927cb495e4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Strategy 3: Compute BERT Embeddings on Fully Cleaned Data\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Returns the BERT embedding of the given text as a NumPy array.\"\"\"\n",
        "    if not text:\n",
        "        return np.zeros(bert_model.get_sentence_embedding_dimension())\n",
        "    return bert_model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "# Compute embeddings on cleaned text\n",
        "patients_df[\"embedding\"] = patients_df[\"combined_conditions\"].apply(get_bert_embedding)\n",
        "trials_df[\"embedding\"] = trials_df[\"text_cleaned\"].apply(get_bert_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"embedding\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "oo_7HTt6AOUH"
      },
      "outputs": [],
      "source": [
        "# Strategy 3: Compute Similarity (Using Fully Cleaned Data)\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4  # Adjusted from 0.6 to improve matching\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n",
        "# Construct the matched patients list\n",
        "matched_patients = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h7y48qNWAQH_",
        "outputId": "aa759bc7-f8cb-4fe2-8b6f-36f8e51fbd61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT-based matching completed. 0 patient-trial pairs found.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 3: Save Results\n",
        "output_df = pd.DataFrame(matched_patients)\n",
        "output_df.to_excel(\"bert_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"bert_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients, json_file, indent=4)\n",
        "\n",
        "print(f\"BERT-based matching completed. {len(matched_patients)} patient-trial pairs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1679d5654acb4637d05de9e3ab03dfa6a4e47a98342068f2230361f0af7a048"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
