{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.49.0)\n",
            "Requirement already satisfied: tqdm in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: scipy in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.29.2)\n",
            "Requirement already satisfied: Pillow in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/brianmmari/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "id": "2SOW0VeL7-6p"
      },
      "outputs": [],
      "source": [
        "# Download all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import ast\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DBAqEo_h-UpO",
        "outputId": "1bec5182-8c9a-4e96-8037-b83b633ce3ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/brianmmari/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 425,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt_tab\")  # Needed for Word2Vec tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "cCLc2E7c-So7"
      },
      "outputs": [],
      "source": [
        "# Don't remove stopwords\n",
        "def clean_text(text):\n",
        "    \"\"\"Keep the text mostly intact.\"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text):\n",
        "        return \"\"\n",
        "    return text.lower()  # Only convert to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "xThrhfH08Cd3"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = os.getcwd() \n",
        "\n",
        "patient_data = pd.read_csv(os.path.join(BASE_DIR, 'raw', 'patients.csv'))\n",
        "\n",
        "diagnosis_data = pd.read_csv(os.path.join(BASE_DIR, 'raw', 'conditions.csv'))\n",
        "\n",
        "# Extract relevant attributes\n",
        "patient_data = patient_data[['Id', 'BIRTHDATE', 'GENDER']]\n",
        "\n",
        "# Convert birthdate to age\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_age(birthdate):\n",
        "    birthdate = datetime.strptime(birthdate, \"%Y-%m-%d\")\n",
        "    today = datetime.today()\n",
        "    return today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "id": "BMOla6iL_NUu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated patient descriptions: 0    [Housing unsatisfactory (finding), Received hi...\n",
            "1    [Serving in military service (finding), Receiv...\n",
            "2    [Medication review due (situation), Traumatic ...\n",
            "3    [Chronic intractable migraine without aura (di...\n",
            "4    [Medication review due (situation), Medication...\n",
            "Name: DESCRIPTION, dtype: object\n"
          ]
        }
      ],
      "source": [
        "patient_data['AGE'] = patient_data['BIRTHDATE'].apply(calculate_age)\n",
        "\n",
        "# Ensure patient IDs are properly formatted\n",
        "patient_data[\"Id\"] = patient_data[\"Id\"].astype(str).str.strip()\n",
        "diagnosis_data[\"PATIENT\"] = diagnosis_data[\"PATIENT\"].astype(str).str.strip()\n",
        "\n",
        "# Group diagnoses for each patient\n",
        "patient_conditions = diagnosis_data.groupby(\"PATIENT\")[\"DESCRIPTION\"].apply(list).reset_index()\n",
        "\n",
        "# Merge with patient data\n",
        "patient_data = patient_data.merge(patient_conditions, left_on=\"Id\", right_on=\"PATIENT\", how=\"left\")\n",
        "\n",
        "# Convert NaN to empty lists\n",
        "patient_data[\"DESCRIPTION\"] = patient_data[\"DESCRIPTION\"].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Debug: Print sample to check if descriptions exist now\n",
        "print(\"Updated patient descriptions:\", patient_data[\"DESCRIPTION\"].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tabjjPNy_miy",
        "outputId": "2cf4a610-7215-49a4-a3ba-223760bc21d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     Id   BIRTHDATE GENDER  AGE  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46  1994-02-06      M   31   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f  1968-08-06      M   56   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668  2008-12-21      M   16   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876  1994-01-27      F   31   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405  2019-07-27      M    5   \n",
            "\n",
            "                                PATIENT  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405   \n",
            "\n",
            "                                         DESCRIPTION  \n",
            "0  [Housing unsatisfactory (finding), Received hi...  \n",
            "1  [Serving in military service (finding), Receiv...  \n",
            "2  [Medication review due (situation), Traumatic ...  \n",
            "3  [Chronic intractable migraine without aura (di...  \n",
            "4  [Medication review due (situation), Medication...  \n",
            "Id             0\n",
            "BIRTHDATE      0\n",
            "GENDER         0\n",
            "AGE            0\n",
            "PATIENT        0\n",
            "DESCRIPTION    0\n",
            "dtype: int64\n",
            "CSV file saved as: /Users/brianmmari/Clinical-Trial-Matching-System/outputs/merged_patient_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Verify the merge\n",
        "print(patient_data.head())\n",
        "print(patient_data.isnull().sum())\n",
        "\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")  # Store outputs in a dedicated folder\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, \"merged_patient_data.csv\")\n",
        "patient_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"CSV file saved as: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "ZfqFVDJ6_rPU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/brianmmari/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and Clean Patient Data EARLY\n",
        "patient_file = \"/Users/brianmmari/Clinical-Trial-Matching-System/outputs/merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# 2. Clean the DESCRIPTION column (ensure it's a list)\n",
        "patients_df[\"DESCRIPTION\"] = patients_df[\"DESCRIPTION\"].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# 3. Download the tokenizer model\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# 4. Tokenize descriptions safely\n",
        "def safe_tokenize(text):\n",
        "    \"\"\"Tokenize and remove stopwords from patient descriptions.\"\"\"\n",
        "    if isinstance(text, list):\n",
        "        text = \" \".join(text)\n",
        "    elif not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set([\"and\", \"the\", \"of\", \"with\", \"for\", \"to\"])\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    \n",
        "    return filtered_tokens\n",
        "\n",
        "patients_df[\"tokenized_description\"] = patients_df[\"DESCRIPTION\"].apply(safe_tokenize)\n",
        "\n",
        "# Merge conditions into a single cleaned text string per patient\n",
        "patients_df[\"combined_conditions\"] = patients_df[\"DESCRIPTION\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "AqYx5eWi_4nC"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load and Clean XML Trial Data EARLY\n",
        "xml_files = glob.glob(os.path.join(BASE_DIR, \"raw\", \"NCT*.xml\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000145.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000151.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000179.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000192.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000193.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000187.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000178.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000150.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000144.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000152.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000146.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000191.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000190.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000147.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000153.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000157.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000143.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000194.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000180.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000195.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000142.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000156.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000168.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000140.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000154.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000197.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000196.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000155.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000169.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000126.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000132.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000133.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000127.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000131.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000125.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000119.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000118.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000124.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000130.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000108.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000134.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000120.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000121.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000135.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000123.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000137.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000136.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000122.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000107.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000113.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000112.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000106.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000110.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000104.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000138.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000139.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000105.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000111.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000129.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000115.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000114.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000128.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000102.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000116.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000117.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000170.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000158.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000159.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000171.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000173.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000167.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000198.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000199.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000172.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000200.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000176.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000162.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000189.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000188.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000163.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000177.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000201.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000149.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000161.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000175.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000174.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000160.xml',\n",
              " '/Users/brianmmari/Clinical-Trial-Matching-System/raw/NCT00000148.xml']"
            ]
          },
          "execution_count": 432,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xml_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "def extract_trial_data(trial_file):\n",
        "    \"\"\"Extract relevant details from each clinical trial XML file.\"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(trial_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract trial ID\n",
        "        trial_id = root.find(\".//id_info/nct_id\")\n",
        "        trial_id = trial_id.text if trial_id is not None else \"Unknown\"\n",
        "\n",
        "        # Extract eligibility criteria\n",
        "        eligibility_criteria = root.find(\".//eligibility/criteria/textblock\")\n",
        "        eligibility_text = eligibility_criteria.text.strip() if eligibility_criteria is not None else \"Missing\"\n",
        "\n",
        "        return {\"Trial_ID\": trial_id, \"text_cleaned\": eligibility_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {trial_file}: {e}\")\n",
        "        return None  # Skip this file if an error occurs\n",
        "\n",
        "# Apply to all XML files\n",
        "trial_data = [extract_trial_data(xml) for xml in xml_files]\n",
        "trial_data = [t for t in trial_data if t is not None]  # Remove failed parses\n",
        "\n",
        "# Convert to DataFrame\n",
        "trials_df = pd.DataFrame(trial_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Series.apply() missing 1 required positional argument: 'func'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[434], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Apply function to trials\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply()\n\u001b[1;32m      4\u001b[0m \u001b[39m# Debug: Print cleaned trial data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(trials_df[\u001b[39m\"\u001b[39m\u001b[39mtext_cleaned\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhead())\n",
            "\u001b[0;31mTypeError\u001b[0m: Series.apply() missing 1 required positional argument: 'func'"
          ]
        }
      ],
      "source": [
        "# Apply function to trials\n",
        "trials_df[\"text_cleaned\"] = trials_df[\"text_cleaned\"].apply()\n",
        "\n",
        "# Debug: Print cleaned trial data\n",
        "print(trials_df[\"text_cleaned\"].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract eligibility criteria **and clean it immediately**\n",
        "    eligibility_criteria = root.find(\".//eligibility/criteria/textblock\")\n",
        "    eligibility_text = clean_text(eligibility_criteria.text) if eligibility_criteria is not None else \"\"\n",
        "\n",
        "    # Extract and clean age range & gender requirements\n",
        "    min_age = root.find(\".//eligibility/minimum_age\")\n",
        "    max_age = root.find(\".//eligibility/maximum_age\")\n",
        "    gender = root.find(\".//eligibility/gender\")\n",
        "\n",
        "    min_age = clean_text(min_age.text) if min_age is not None else \"0 years\"\n",
        "    max_age = clean_text(max_age.text) if max_age is not None else \"100 years\"\n",
        "    gender = clean_text(gender.text) if gender is not None else \"all\"\n",
        "\n",
        "    # Convert age range into numerical values\n",
        "    def extract_age(age_text):\n",
        "        return int(age_text.split()[0]) if \"years\" in age_text else 0\n",
        "\n",
        "    min_age = extract_age(min_age)\n",
        "    max_age = extract_age(max_age)\n",
        "\n",
        "    # Extract inclusion and exclusion criteria (and clean them)\n",
        "    inclusion_criteria = []\n",
        "    exclusion_criteria = []\n",
        "    parsing_exclusion = False\n",
        "\n",
        "    for line in eligibility_text.split(\"\\n\"):\n",
        "        line = clean_text(line.strip())  # Ensure each line is cleaned\n",
        "        if \"exclusion\" in line:\n",
        "            parsing_exclusion = True\n",
        "        elif \"inclusion\" in line:\n",
        "            parsing_exclusion = False\n",
        "        elif line:\n",
        "            if parsing_exclusion:\n",
        "                exclusion_criteria.append(line)\n",
        "            else:\n",
        "                inclusion_criteria.append(line)\n",
        "\n",
        "    return min_age, max_age, gender, inclusion_criteria, exclusion_criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "NO4iEWs6_8dI"
      },
      "outputs": [],
      "source": [
        "# Strategy 1: Process String-Based Matching on Cleaned Data\n",
        "def is_eligible(patient, min_age, max_age, gender, inclusion_criteria, exclusion_criteria):\n",
        "    \"\"\"Check if a patient is eligible for a trial based on cleaned conditions and criteria.\"\"\"\n",
        "    # Check age\n",
        "    if not (min_age <= patient[\"AGE\"] <= max_age):\n",
        "        return False\n",
        "\n",
        "    # Check gender\n",
        "    if gender != \"All\" and patient[\"GENDER\"] != gender:\n",
        "        return False\n",
        "\n",
        "    # Clean inclusion/exclusion criteria\n",
        "    inclusion_criteria = [clean_text(inc) for inc in inclusion_criteria]\n",
        "    exclusion_criteria = [clean_text(exc) for exc in exclusion_criteria]\n",
        "\n",
        "    # Convert patient conditions to cleaned string\n",
        "    patient_conditions = \" \".join(patient[\"DESCRIPTION\"]).lower()\n",
        "\n",
        "    # Inclusion: At least one criterion should match\n",
        "    if not any(inc in patient_conditions for inc in inclusion_criteria[:3]):  \n",
        "        return False  # Match at least 3 criteria\n",
        "\n",
        "    # Exclusion: No disqualifying criteria should be present\n",
        "    if any(exc in patient_conditions for exc in exclusion_criteria):\n",
        "        return False  # Disqualified\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aKD4CdiDB8_N",
        "outputId": "788ecd1c-96ca-470d-ade4-3b633724bf17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No eligible patients found. Skipping CSV export.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 1: Process each XML file\n",
        "all_eligible_patients = []\n",
        "if all_eligible_patients:\n",
        "    final_eligible_patients_df = pd.concat(all_eligible_patients, ignore_index=True)\n",
        "    final_eligible_patients_df = final_eligible_patients_df.groupby([\"Id\", \"AGE\", \"GENDER\"])[\"Trial_ID\"].apply(list).reset_index()\n",
        "\n",
        "    # Save to file\n",
        "    output_file = \"eligible_patients.csv\"\n",
        "    final_eligible_patients_df.to_csv(output_file, index=False)\n",
        "    print(f\"Eligible patients saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No eligible patients found. Skipping CSV export.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "id": "2gz-rCpgACWx"
      },
      "outputs": [],
      "source": [
        "sentences = [word_tokenize(\" \".join(desc)) for desc in patients_df['DESCRIPTION']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add cleaned inclusion criteria for trials\n",
        "for xml_file in xml_files:\n",
        "    _, _, _, inclusion_criteria, _ = extract_criteria(xml_file)  # Extract only inclusion criteria\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(word_tokenize(\" \".join(inclusion_criteria)))  # Convert list to string before tokenizing\n",
        "\n",
        "# Train Word2Vec on cleaned text\n",
        "w2v_model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model.build_vocab(sentences)\n",
        "w2v_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "# Save the model\n",
        "w2v_model.save(\"word2vec_patient_trials.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {
        "id": "yDxw8Iv3ClYJ"
      },
      "outputs": [],
      "source": [
        "# Strategy 2: Compute Patient & Trial Embeddings\n",
        "def get_w2v_embedding(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # Use zero vector if no words found\n",
        "\n",
        "# Compute embeddings for all patients and trials\n",
        "patients_df[\"w2v_embedding\"] = patients_df[\"combined_conditions\"].apply(get_w2v_embedding)\n",
        "trials_df[\"w2v_embedding\"] = trials_df[\"text_cleaned\"].apply(get_w2v_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"w2v_embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"w2v_embedding\"].values)\n",
        "\n",
        "# Step 5: Compute Cosine Similarity\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4  # Match the BERT threshold\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {
        "id": "8i6QsLRaJWxc"
      },
      "outputs": [],
      "source": [
        "# Strategy 2: Construct the Output in eligible Format\n",
        "matched_patients_w2v = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6EXqoAgLJSQI",
        "outputId": "07927f1c-0300-41c5-c868-9e069c79cae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec-based matching completed. 0 patient-trial pairs found.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 2: Save Results in BERT Format\n",
        "output_df_w2v = pd.DataFrame(matched_patients_w2v)\n",
        "output_df_w2v.to_excel(\"word2vec_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"word2vec_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients_w2v, json_file, indent=4)\n",
        "\n",
        "print(f\"Word2Vec-based matching completed. {len(matched_patients_w2v)} patient-trial pairs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {
        "id": "yVp6zkl9GHSk"
      },
      "outputs": [],
      "source": [
        "# Load all XML files\n",
        "def extract_inclusion_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "    text = root.find(\".//eligibility/criteria/textblock\")\n",
        "    return word_tokenize(text.text.lower()) if text is not None else []\n",
        "\n",
        "# Add inclusion criteria to Word2Vec training data\n",
        "for xml_file in xml_files:\n",
        "    inclusion_criteria = extract_inclusion_criteria(xml_file)\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(inclusion_criteria)  # Include trial criteria in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {
        "id": "LBvfqXrLAMHO"
      },
      "outputs": [],
      "source": [
        "# Strategy 3: Compute BERT Embeddings on Fully Cleaned Data\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Returns the BERT embedding of the given text as a NumPy array.\"\"\"\n",
        "    if not text:\n",
        "        return np.zeros(bert_model.get_sentence_embedding_dimension())\n",
        "    return bert_model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "# Compute embeddings on cleaned text\n",
        "patients_df[\"embedding\"] = patients_df[\"combined_conditions\"].apply(get_bert_embedding)\n",
        "trials_df[\"embedding\"] = trials_df[\"text_cleaned\"].apply(get_bert_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"embedding\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {
        "id": "oo_7HTt6AOUH"
      },
      "outputs": [],
      "source": [
        "# Strategy 3: Compute Similarity (Using Fully Cleaned Data)\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n",
        "# Construct the matched patients list\n",
        "matched_patients = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h7y48qNWAQH_",
        "outputId": "aa759bc7-f8cb-4fe2-8b6f-36f8e51fbd61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT-based matching completed. 0 patient-trial pairs found.\n"
          ]
        }
      ],
      "source": [
        "# Strategy 3: Save Results\n",
        "output_df = pd.DataFrame(matched_patients)\n",
        "output_df.to_excel(\"bert_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"bert_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients, json_file, indent=4)\n",
        "\n",
        "print(f\"BERT-based matching completed. {len(matched_patients)} patient-trial pairs found.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1679d5654acb4637d05de9e3ab03dfa6a4e47a98342068f2230361f0af7a048"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
