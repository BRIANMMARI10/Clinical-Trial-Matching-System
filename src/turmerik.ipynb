{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "vIFokhxqlHhW"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_data = pd.read_csv('/content/patients.csv')\n",
        "\n",
        "# Extract relevant attributes\n",
        "patient_data = patient_data[['Id', 'BIRTHDATE', 'GENDER']]\n",
        "\n",
        "# Convert birthdate to age\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_age(birthdate):\n",
        "    birthdate = datetime.strptime(birthdate, \"%Y-%m-%d\")\n",
        "    today = datetime.today()\n",
        "    return today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))\n",
        "\n",
        "patient_data['AGE'] = patient_data['BIRTHDATE'].apply(calculate_age)\n",
        "\n",
        "# Ensure Id is clean and string type\n",
        "patient_data['Id'] = patient_data['Id'].astype(str).str.strip()\n",
        "\n",
        "diagnosis_data = pd.read_csv('/content/conditions.csv')\n",
        "# Ensure Patient is clean and string type\n",
        "diagnosis_data['PATIENT'] = diagnosis_data['PATIENT'].astype(str).str.strip()"
      ],
      "metadata": {
        "id": "9zL-7A1blpZl"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to check whether they both have Patient_ID to ensure effective merging\n",
        "# Check data types before merging\n",
        "print(\"Patient data: \", patient_data.dtypes)\n",
        "print(\"Diagnosis data: \", diagnosis_data.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOLfpoW7zqTQ",
        "outputId": "11ad80db-bbed-403b-bd57-e744dfc1db80"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient data:  Id           object\n",
            "BIRTHDATE    object\n",
            "GENDER       object\n",
            "AGE           int64\n",
            "dtype: object\n",
            "Diagnosis data:  START          object\n",
            "STOP           object\n",
            "PATIENT        object\n",
            "ENCOUNTER      object\n",
            "SYSTEM         object\n",
            "CODE            int64\n",
            "DESCRIPTION    object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate conditions for each patient\n",
        "patient_conditions = diagnosis_data.groupby('PATIENT')['DESCRIPTION'].apply(list).reset_index()\n",
        "\n",
        "# Merge patient conditions\n",
        "patient_data = patient_data.merge(patient_conditions, left_on='Id', right_on='PATIENT', how='left')\n",
        "\n",
        "# Convert NaN conditions to empty lists\n",
        "patient_data['DESCRIPTION'] = patient_data['DESCRIPTION'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Verify the merge\n",
        "print(patient_data.head())\n",
        "print(patient_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhjvY23p1G4X",
        "outputId": "c60f0c83-65ae-4c7a-a9a1-1711869c825d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Id   BIRTHDATE GENDER  AGE  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46  1994-02-06      M   31   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f  1968-08-06      M   56   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668  2008-12-21      M   16   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876  1994-01-27      F   31   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405  2019-07-27      M    5   \n",
            "\n",
            "                                PATIENT  \\\n",
            "0  30a6452c-4297-a1ac-977a-6a23237c7b46   \n",
            "1  34a4dcc4-35fb-6ad5-ab98-be285c586a4f   \n",
            "2  7179458e-d6e3-c723-2530-d4acfe1c2668   \n",
            "3  37c177ea-4398-fb7a-29fa-70eb3d673876   \n",
            "4  0fef2411-21f0-a269-82fb-c42b55471405   \n",
            "\n",
            "                                         DESCRIPTION  \n",
            "0  [Housing unsatisfactory (finding), Received hi...  \n",
            "1  [Serving in military service (finding), Receiv...  \n",
            "2  [Medication review due (situation), Traumatic ...  \n",
            "3  [Chronic intractable migraine without aura (di...  \n",
            "4  [Medication review due (situation), Medication...  \n",
            "Id             0\n",
            "BIRTHDATE      0\n",
            "GENDER         0\n",
            "AGE            0\n",
            "PATIENT        0\n",
            "DESCRIPTION    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"merged_patient_data.csv\"\n",
        "patient_data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"CSV file saved as: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wpAPH8B1eEM",
        "outputId": "88421647-6612-458b-d2b3-652b3bdca408"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved as: merged_patient_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Load patient data\n",
        "patient_file = \"merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# Convert conditions from string to list if needed\n",
        "patients_df['DESCRIPTION'] = patients_df['DESCRIPTION'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "print(\"Patient data loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dofiWDzf-DY-",
        "outputId": "ba0e73ce-c882-43c6-b0c7-7a1e476e73e9"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient data loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "\n",
        "# Get XML files from NCT00000102.xml to NCT00000300.xml\n",
        "xml_files = glob.glob(\"/content/NCT*.xml\")\n",
        "\n",
        "# Function to extract criteria from XML\n",
        "def extract_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract eligibility criteria\n",
        "    eligibility_criteria = root.find(\".//eligibility/criteria/textblock\").text if root.find(\".//eligibility/criteria/textblock\") is not None else \"\"\n",
        "\n",
        "    # Extract age range and gender requirements\n",
        "    min_age = root.find(\".//eligibility/minimum_age\").text if root.find(\".//eligibility/minimum_age\") is not None else \"0 Years\"\n",
        "    max_age = root.find(\".//eligibility/maximum_age\").text if root.find(\".//eligibility/maximum_age\") is not None else \"100 Years\"\n",
        "    gender = root.find(\".//eligibility/gender\").text if root.find(\".//eligibility/gender\") is not None else \"All\"\n",
        "\n",
        "    # Convert age range into numerical values\n",
        "    def extract_age(age_text):\n",
        "        return int(age_text.split()[0]) if \"Years\" in age_text else 0\n",
        "\n",
        "    min_age = extract_age(min_age)\n",
        "    max_age = extract_age(max_age)\n",
        "\n",
        "    # Extract inclusion and exclusion criteria separately\n",
        "    inclusion_criteria = []\n",
        "    exclusion_criteria = []\n",
        "    parsing_exclusion = False\n",
        "\n",
        "    for line in eligibility_criteria.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if \"Exclusion\" in line:\n",
        "            parsing_exclusion = True\n",
        "        elif \"Inclusion\" in line:\n",
        "            parsing_exclusion = False\n",
        "        elif line:\n",
        "            if parsing_exclusion:\n",
        "                exclusion_criteria.append(line)\n",
        "            else:\n",
        "                inclusion_criteria.append(line)\n",
        "\n",
        "    return min_age, max_age, gender, inclusion_criteria, exclusion_criteria\n",
        "\n",
        "print(\"Clinical trial extraction function is ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiM-umL6lu7a",
        "outputId": "fad00e29-4a7f-4a0e-e3ce-2cf439f89319"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clinical trial extraction function is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if a patient is eligible\n",
        "# String-based matching,\n",
        "def is_eligible(patient, min_age, max_age, gender, inclusion_criteria, exclusion_criteria):\n",
        "    # Check age eligibility\n",
        "    if not (min_age <= patient[\"AGE\"] <= max_age):\n",
        "        return False\n",
        "\n",
        "    # Check gender eligibility\n",
        "    if gender != \"All\" and patient[\"GENDER\"] != gender:\n",
        "        return False\n",
        "\n",
        "    # Convert patient conditions into a single lowercase string\n",
        "    patient_conditions = \" \".join(patient[\"DESCRIPTION\"]).lower()\n",
        "\n",
        "    # **Looser matching**: If any inclusion criterion **partially matches**, accept the patient\n",
        "    if not any(any(word in patient_conditions for word in inc.lower().split()) for inc in inclusion_criteria):\n",
        "        return False  # Patient does not meet ANY inclusion criteria\n",
        "\n",
        "    # **Exclusion criteria**: Ensure no disqualifying conditions are present\n",
        "    if any(exc.lower() in patient_conditions for exc in exclusion_criteria):\n",
        "        return False  # Patient has an exclusion condition\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "nR9iFb-Kvst1"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each XML file\n",
        "all_eligible_patients = []\n",
        "for xml_file in xml_files:\n",
        "    min_age, max_age, gender, inclusion_criteria, exclusion_criteria = extract_criteria(xml_file)\n",
        "\n",
        "    eligible_patients_df = patients_df[\n",
        "        patients_df.apply(lambda p: is_eligible(p, min_age, max_age, gender, inclusion_criteria, exclusion_criteria), axis=1)\n",
        "    ]\n",
        "\n",
        "    if not eligible_patients_df.empty:  # Only modify if not empty\n",
        "        eligible_patients_df = eligible_patients_df.copy()  # Ensure safe modification\n",
        "        eligible_patients_df.loc[:, \"Trial_ID\"] = xml_file.split(\"/\")[-1].replace(\".xml\", \"\")\n",
        "        all_eligible_patients.append(eligible_patients_df)  # Append only non-empty results\n",
        "\n",
        "\n",
        "# Combine results from all trials\n",
        "final_eligible_patients_df = pd.concat(all_eligible_patients, ignore_index=True)\n",
        "\n",
        "final_eligible_patients_df = final_eligible_patients_df.groupby([\"Id\", \"AGE\", \"GENDER\"])[\"Trial_ID\"].apply(list).reset_index()\n",
        "\n",
        "# Save eligible patients to a new file\n",
        "output_file = \"eligible_patients.csv\"\n",
        "final_eligible_patients_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Eligible patients saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813vaWRJVCPO",
        "outputId": "0c89df6d-4dc4-4525-c479-2d2fc6ebeea0"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eligible patients saved to eligible_patients.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I'm gonna use Word2Vec to calculate similarity scores\n",
        "!pip install gensim\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')  # Ensure word tokenization works"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJQ39tVqAfYp",
        "outputId": "cbcea2f3-b726-483a-da28-5c626407e7c7"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load patient data\n",
        "patient_file = \"merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# Convert condition descriptions from strings to lists\n",
        "patients_df['DESCRIPTION'] = patients_df['DESCRIPTION'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Tokenize descriptions (Word2Vec requires tokenized text)\n",
        "sentences = [word_tokenize(\" \".join(desc)) for desc in patients_df['DESCRIPTION']]"
      ],
      "metadata": {
        "id": "13WluOAgDh1Z"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all XML files\n",
        "def extract_inclusion_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "    text = root.find(\".//eligibility/criteria/textblock\")\n",
        "    return word_tokenize(text.text.lower()) if text is not None else []\n",
        "\n",
        "# Add inclusion criteria to Word2Vec training data\n",
        "for xml_file in xml_files:\n",
        "    inclusion_criteria = extract_inclusion_criteria(xml_file)\n",
        "    if inclusion_criteria:\n",
        "        sentences.append(inclusion_criteria)  # Include trial criteria in training data"
      ],
      "metadata": {
        "id": "qFNOPHZ1Dr8C"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model for future use\n",
        "w2v_model.save(\"word2vec_patient_trials.model\")"
      ],
      "metadata": {
        "id": "Bsjbkj0tGFQM"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute similarity score between two texts using Word2Vec\n",
        "def compute_similarity(patient_conditions, trial_criteria):\n",
        "    patient_tokens = word_tokenize(\" \".join(patient_conditions).lower())\n",
        "    trial_tokens = word_tokenize(\" \".join(trial_criteria).lower())\n",
        "\n",
        "    # Get vector representations (ignore words not in vocab)\n",
        "    patient_vectors = [w2v_model.wv[word] for word in patient_tokens if word in w2v_model.wv]\n",
        "    trial_vectors = [w2v_model.wv[word] for word in trial_tokens if word in w2v_model.wv]\n",
        "\n",
        "    if not patient_vectors or not trial_vectors:\n",
        "        return 0  # No meaningful comparison possible\n",
        "\n",
        "    # Compute average vector for both\n",
        "    patient_avg_vector = np.mean(patient_vectors, axis=0)\n",
        "    trial_avg_vector = np.mean(trial_vectors, axis=0)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = np.dot(patient_avg_vector, trial_avg_vector) / (\n",
        "        np.linalg.norm(patient_avg_vector) * np.linalg.norm(trial_avg_vector)\n",
        "    )\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "3tcKswKlGHvL"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ALsSKSINWV",
        "outputId": "b860dbbf-4cc6-42b9-ef61-3b2b01b3dffb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set similarity threshold (0.6 is moderate, 0.8 is strong)\n",
        "SIMILARITY_THRESHOLD = 0.6\n",
        "\n",
        "# Create a list to store eligible patients\n",
        "matched_patients = []\n",
        "\n",
        "# Iterate over trials and match patients\n",
        "for xml_file in xml_files:\n",
        "    inclusion_criteria = extract_inclusion_criteria(xml_file)\n",
        "\n",
        "    for _, patient in patients_df.iterrows():\n",
        "        similarity_score = compute_similarity(patient[\"DESCRIPTION\"], inclusion_criteria)\n",
        "\n",
        "        if similarity_score >= SIMILARITY_THRESHOLD:\n",
        "            matched_patients.append({\n",
        "                \"Patient_ID\": patient[\"Id\"],\n",
        "                \"Age\": patient[\"AGE\"],\n",
        "                \"Gender\": patient[\"GENDER\"],\n",
        "                \"Trial_ID\": xml_file.split(\"/\")[-1].replace(\".xml\", \"\"),\n",
        "                \"Similarity_Score\": similarity_score\n",
        "            })\n",
        "\n",
        "# Convert to DataFrame\n",
        "matched_patients_df = pd.DataFrame(matched_patients)\n",
        "\n",
        "# Aggregate Trial_IDs with their respective Similarity Scores in the format \"Trial_ID[similarity_score]\"\n",
        "aggregated_df = matched_patients_df.groupby([\"Patient_ID\", \"Age\", \"Gender\"]).apply(\n",
        "    lambda x: [f\"{trial_id}[{similarity_score:.4f}]\" for trial_id, similarity_score in zip(x[\"Trial_ID\"], x[\"Similarity_Score\"])]\n",
        ").reset_index(name=\"Trial_Matches\")\n",
        "\n",
        "# Save the results to an Excel file\n",
        "aggregated_df.to_excel(\"word2vec_matched_patients.xlsx\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeTyK86WGJ3g",
        "outputId": "67b6d813-aa80-40ed-884a-e83c083a898d"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-138-6fcfbc112384>:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  aggregated_df = matched_patients_df.groupby([\"Patient_ID\", \"Age\", \"Gender\"]).apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we use BERT\n",
        "!pip install transformers torch sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXuo7QY_gs0r",
        "outputId": "8113d0f5-18f5-460b-98e6-009c71c5f3ed"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a fast, efficient BERT-based model\n",
        "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to get BERT embedding\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Returns the BERT embedding of the given text as a NumPy array.\"\"\"\n",
        "    if not isinstance(text, str) or pd.isna(text):\n",
        "        return np.zeros(bert_model.get_sentence_embedding_dimension())\n",
        "    return bert_model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "# Load patient data\n",
        "patient_file = \"merged_patient_data.csv\"\n",
        "patients_df = pd.read_csv(patient_file)\n",
        "\n",
        "# Convert DESCRIPTION column to list if stored as a string\n",
        "patients_df['DESCRIPTION'] = patients_df['DESCRIPTION'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
        "\n",
        "# Convert patient conditions into a single text string\n",
        "patients_df['combined_conditions'] = patients_df['DESCRIPTION'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Load all XML trial files\n",
        "xml_files = glob.glob(\"/content/NCT*.xml\")\n",
        "\n",
        "# Function to extract eligibility criteria from XML\n",
        "def extract_criteria(trial_file):\n",
        "    tree = ET.parse(trial_file)\n",
        "    root = tree.getroot()\n",
        "    text = root.find(\".//eligibility/criteria/textblock\")\n",
        "    return text.text if text is not None else \"\"\n",
        "\n",
        "# Extract and store trial criteria\n",
        "trials_data = []\n",
        "for xml_file in xml_files:\n",
        "    criteria_text = extract_criteria(xml_file)\n",
        "    if criteria_text:\n",
        "        trials_data.append({\"Trial_ID\": xml_file.split(\"/\")[-1].replace(\".xml\", \"\"), \"text_cleaned\": criteria_text})\n",
        "\n",
        "trials_df = pd.DataFrame(trials_data)\n",
        "\n",
        "### 🔹 **Place Text Preprocessing Here (Before Encoding!)**\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by converting to lowercase, removing punctuation, and removing stopwords.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "patients_df[\"combined_conditions\"] = patients_df[\"combined_conditions\"].apply(clean_text)\n",
        "trials_df[\"text_cleaned\"] = trials_df[\"text_cleaned\"].apply(clean_text)\n",
        "\n",
        "### Now Compute BERT Embeddings (On Cleaned Text)**\n",
        "patients_df[\"embedding\"] = patients_df[\"combined_conditions\"].apply(get_bert_embedding)\n",
        "trials_df[\"embedding\"] = trials_df[\"text_cleaned\"].apply(get_bert_embedding)\n",
        "\n",
        "# Convert embeddings to NumPy arrays\n",
        "patient_embeddings = np.vstack(patients_df[\"embedding\"].values)\n",
        "trial_embeddings = np.vstack(trials_df[\"embedding\"].values)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity_matrix = cosine_similarity(patient_embeddings, trial_embeddings)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.4  # Adjusted from 0.6 to improve matching\n",
        "\n",
        "# Find matches using NumPy filtering\n",
        "patient_indices, trial_indices = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n",
        "# Construct the matched patients list\n",
        "matched_patients = [\n",
        "    {\n",
        "        \"patientId\": patients_df.iloc[p_idx][\"Id\"],\n",
        "        \"trialId\": trials_df.iloc[t_idx][\"Trial_ID\"],\n",
        "        \"trialName\": f\"Trial {trials_df.iloc[t_idx]['Trial_ID']}\",\n",
        "        \"eligibilityCriteriaMet\": [f\"{trials_df.iloc[t_idx]['Trial_ID']}[{similarity_matrix[p_idx, t_idx]:.4f}]\"]\n",
        "    }\n",
        "    for p_idx, t_idx in zip(patient_indices, trial_indices)\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDenj4YV5kls",
        "outputId": "8f602c61-8164-49d2-c23f-2f1e85fd2271"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT-based matching completed. 1763 patient-trial pairs found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "output_df = pd.DataFrame(matched_patients)\n",
        "output_df.to_excel(\"bert_matched_patients.xlsx\", index=False)\n",
        "\n",
        "with open(\"bert_matched_patients.json\", \"w\") as json_file:\n",
        "    json.dump(matched_patients, json_file, indent=4)\n",
        "\n",
        "print(f\"BERT-based matching completed. {len(matched_patients)} patient-trial pairs found.\")"
      ],
      "metadata": {
        "id": "1G6EYO6n5BIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}